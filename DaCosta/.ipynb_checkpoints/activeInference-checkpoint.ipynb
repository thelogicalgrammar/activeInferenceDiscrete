{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.special import digamma, softmax\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is meant as a partial re-implementation in numpy of the model presented in the paper \"Active Inference on Discrete State-Spaces - A synthesis\" by Da Costa et al., which can be found [here](https://arxiv.org/pdf/2001.07203.pdf). The paper is really great, but it's pretty difficult at points, so to help me understand the passages clearly I thought of taking notes and organizing them here. Some bits that I found difficult are explained a bit more slowly here than they are in the paper. In other parts I just assume what they say in the paper.\n",
    "\n",
    "I am doing this cause I had a bit of free time in the last few days, but I will get busy again soon. So if somebody wants to expand this more, please let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by reviewing the components of the model:\n",
    "\n",
    "- $s_\\tau$: a one-hot vector modelling the state at time $\\tau$. The vector is 1 on the index of the state. The number of components is $m$.\n",
    "- $o_\\tau$: like $s_\\tau$, but for observations. Number of components is $n$.\n",
    "- $\\pi$ is a policy, which is just a vector whose $i^{th}$ element, $\\pi_i$, contains the action to be performed at timestep $i$. [Is there a reason this is $i$ rather than $\\tau$?]\n",
    "- $A$: A likelihood matrix with shape $n \\times m$. This is used in the generative model to model how states generate observations. Each column corresponds to a state, and each row to an observation. Each column is a probability vector over observations, describing the probabilities that the state corresponding to the column would produce each row/observation. $o_\\tau^T A$ is the vector of the probabilities of $o_\\tau$ being generated by each state. $A s_\\tau$ is the vector with the probability of each observation given state $s_\\tau$. $o_\\tau^T A s_\\tau$ is the probability that state $s_\\tau$ would generate observation $o_\\tau$. \n",
    "- $a$: a matrix encoding the hyperprior over likelihood matrices $A$. The $j^{th}$ column of $a$, $a_{\\bullet, j}$, contains the $\\alpha$ vector to parameterize a Dirichlet distribution. In practice, each column of $a$ contains the parameters to sample the vector of probabilities of each observation given the state corresponding to the column.\n",
    "- $B$: Transition matrix between states which depends on the agent's action. It is indexed by an action, e.g. $B_{\\pi_i}$ (So really, it's a 3-d array, which at each action-index has an $m \\times m$ transition matrix). The $i^{th}$ column of $B_j$ contains the probabilities of transitioning to each possible state in the time step. So $B_{\\pi_{i-1}} s_{\\tau - 1}$ is the vector of probabilities of transitioning to each state at time $\\tau$, given the action indicated by the policy at time $i-1$, $\\pi_{i-1}$, and the state at time $s_{\\tau-1}$.\n",
    "- $C$: Parameters for categorical hyperprior over states or outcomes, depending on how you want to specify the agent's preferences. It encodes prior probabilities that effectively work like preferences for states or sensory input in the free energy framework.\n",
    "- $D$: Parameters for categorical hyperprior over first state. \n",
    "- $E$: Parameters for categorical hyperprior over policies.\n",
    "\n",
    "And now the generative model in all its beauty, screenshotted from the paper:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/generativeModelDescription.png\" width=\"400\">\n",
    "\n",
    "<img src=\"img/generativeModelGraph.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generative model tells us the joint probability of likelihood matrix ($A$), policy ($\\pi$), a sequence of states ($s_{1:T}$), and a sequence of observations ($o_{1:T}$). It is called **generative** because we can use it to generate possible histories by successively sampling (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: the shocking bracelet\n",
    "Suppose you are wearing a bracelet that can deliver a small electric shock at discrete time intervals. Whether or not you are shocked counts as an observation, $o$:\n",
    "- $o_1$: you are shocked\n",
    "- $o_2$: you are not shocked\n",
    "\n",
    "We assume the bracelet has a hidden state, $s$, that has some influence over whether or not you are shocked:\n",
    "- $s_1$: bracelet hidden state 1\n",
    "- $s_2$: bracelet hidden state 2\n",
    "\n",
    "The bracelet has a little button on it. You can perform an action, $u$, which is either pressing or not pressing the button:\n",
    "- $u_1$: you press the button\n",
    "- $u_2$: you don't press the button\n",
    "\n",
    "In this setup, $A$ determines how the hidden state causes a shock, while $B$ determines how pressing the button affects the hidden state. The policy $\\pi$ is a sequence of button presses, so at a particular time $i$ the policy determines an action: $\\pi_i=u$.\n",
    "\n",
    "Let's initalise this model and generate some sequences of hidden states and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise(     \n",
    "        n_states=2,     # hidden states of the bracelet\n",
    "        n_actions=2,    # press/don't press the button\n",
    "        n_obs=2,        # shock/no shock\n",
    "        T=10,           # number of trials (10 shocks, or 10 no shocks, or a mixture)\n",
    "        policy_setting=\"essential\"   # see below\n",
    "        ):\n",
    "    \"\"\"\n",
    "        The generative model tells us the joint probability of likelihood matrix A, \n",
    "         policy π, states S, and observations O. \n",
    "        It is called generative because we can use it to generate possible histories \n",
    "         by successively sampling (see generate_history()).\n",
    "        \n",
    "        Reference to node numbers are from Da Costa et al's figure 2, page 9.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## 1. Get policy.\n",
    "    ##     A policy π is a sequence of acts u.\n",
    "    if policy_setting == \"exhaustive\":\n",
    "        ## sample pi (node 1). This is not the most efficient implementation, but it is the clearest.\n",
    "        ## create set of possible policies, i.e. set of action indices of length T\n",
    "        set_policies = list(product(range(n_actions), repeat=T))\n",
    "        \n",
    "        ## create the hyperprior over policies\n",
    "        E = np.random.dirichlet(alpha=[1]*len(set_policies))\n",
    "        \n",
    "        ## sample the index of the policy to select from set_policies\n",
    "        index_policy = np.random.choice(len(set_policies), p=E) # select policy\n",
    "        pi = set_policies[index_policy]\n",
    "    \n",
    "    elif policy_setting == \"essential\":\n",
    "        ## A random list of integers chosen from 0 to n_actions, length T\n",
    "        pi = np.random.randint(0, n_actions, size=T) # select policy\n",
    "\n",
    "    ## 2. Sample A (node 4)\n",
    "    ## A is the conditional matrix p(o|s) of state-observation probabilities.\n",
    "    ## (i.e. the likelihood of the state given the observation.)\n",
    "    ## You can think of it as a matrix-valued random variable.\n",
    "    ## 2.a. Sample hyperparameters:\n",
    "    a = stats.halfnorm.rvs(size=(n_obs, n_states))\n",
    "    \n",
    "    ## 2.b. For each column in the hyperparameters matrix a, sample from a dirichlet\n",
    "    ## (then transpose because each dirichlet sample creates a row, instead of a column)\n",
    "    A = np.array([np.random.dirichlet(a[:,i]) for i in range(a.shape[1])]).T\n",
    "\n",
    "    ## 3. Sample s_1 (node 5)\n",
    "    D = np.random.dirichlet(alpha=[1]*n_states)\n",
    "    \n",
    "    ## 4. Calculate B (node 3)\n",
    "    ## B is the state-state transitions. It is influenced by pi,\n",
    "    ##  because your actions influence how external states flow between each other.\n",
    "    ## for simplicity we code B as a 3-d array with dimensions (n_actions, n_states, n_states).\n",
    "    ## First create an array with totally random elements >0 and then normalize over the 2nd dimension\n",
    "    ##  so that each row sums to 1\n",
    "    B = np.random.rand(n_actions, n_states, n_states)\n",
    "    B /= B.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    return n_states, n_obs, T, B, pi, A, D, a\n",
    "    \n",
    "\n",
    "def generate_history(n_states, n_obs, T, B, pi, A, D):\n",
    "    \"\"\"\n",
    "        Successive sampling of the generative process.\n",
    "        \n",
    "        NB The generative *model* is inside the agent's head.\n",
    "           The generative *process* is the actual process out in the world that\n",
    "            produces sensory samples O from hidden states S.\n",
    "        \n",
    "        We assume the structure of both the model and the process is the same,\n",
    "         which is why we use components of the model (i.e. the matrices A and B)\n",
    "         to stand in for the process generating s_history and o_history.\n",
    "        \n",
    "        Presumably there is a way to frame the inference problem\n",
    "         so that even the entries in A and B must be learned.\n",
    "        For now, we focus on the simpler problem that assumes\n",
    "         we already have the correct entries.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## 1. Initialise\n",
    "    s_history, o_history= [], []\n",
    "    \n",
    "    ## 2. First state\n",
    "    s = np.random.choice(n_states, p=D)\n",
    "\n",
    "    ## 3. at each time step...\n",
    "    for i in range(T):\n",
    "        s_history.append(s)\n",
    "        \n",
    "        ## ...sample an observation given A and state s...\n",
    "        o = np.random.choice(n_obs, p=A[:,s])\n",
    "        o_history.append(o)\n",
    "        \n",
    "        ## ...and update state s_i given policy, B, and previous state.\n",
    "        ##  (So it looks like B doesn't change throughout the run.)\n",
    "        s = np.random.choice(n_states, p=B[pi[i],:,s])\n",
    "\n",
    "        \n",
    "    return o_history, s_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example should produce a list of ten lists, each of which contains one observation (0 or 1) and one hidden state (0 or 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "model_input = initialise()\n",
    "o_history, s_history = generate_history(*model_input[:-1])\n",
    "print(np.column_stack((o_history, s_history)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferring the hidden state from the observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, it is easy to produce imagined histories from the generative model. What's difficult is to _invert_ the generative model. That means to observe data $o$ and create a posterior distribution $p(s|o)$ for the values of the unobserved variables in the model.\n",
    "\n",
    "The point of Variational Bayesian methods is to give us a way to **estimate the posterior from the generative model and the data**. And that's of course the tricky bit! So instead of using the original generative model, we use another model whose variables we estimate to be as close as possible (in KL-divergence) to the unknown posterior. Before we see that, let me introduce a few more symbols (all of which are **bolded** to distinguish them from non-bolded symbols that use the same letters):\n",
    "- $\\boldsymbol{\\pi}$: A vector of probabilities that parameterize the categorical distribution from which policies $\\pi$ (non-bolded) are sampled.\n",
    "- $\\mathbf{s}$: This is a 3-d array which, for every combination of policy and time, gives us a vector of the probabilities of each state. The dimensions are (num policies, T, n_states).\n",
    "- $\\mathbf{a}$: Much like $a$ above, each column parameterizes a Dirichlet distribution corresponding to a state. A sample from the Dirichlet corresponding to a state is the parameters of a categorical distribution over observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, the beautiful approximated model, which calculates the joint probability of the history of states and the matrix encoding the transition probabilities between states (_assuming a certain policy_) just in terms of a distribution over states for each timestep. Basically, only the essentials:\n",
    "\n",
    "<img src=\"img/approximatedPosterior.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In glorious active inference tradition, perception is thought of as inference about hidden states from sensation. This is done by minimizing the free energy with respect to the hidden states in the approximate posterior for every policy. This gives the best approximation to the true posterior in terms of the hidden states for all timesteps. How to do this is derived in equations (5), (6), and (7) of the paper. What I found a little bit harder to understand is the passage from (6) to (7), so I am going to review it a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider first the first summand transformed in terms of the sufficient parameters of $Q$, contained in $\\mathbf{s}$:\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{\\tau=1}^T \\mathbb{E}_{Q(s_\\tau \\mid \\pi)}\\left[ \\log Q\\left( s_\\tau \\mid \\pi \\right) \\right] & \\implies \\sum_{\\tau=1}^T \\mathbf{s}_{\\pi \\tau} \\cdot \\log \\mathbf{s}_{\\pi \\tau}\n",
    "\\end{align}\n",
    "\n",
    "$Q(s_\\tau \\mid \\pi)$ is a distribution over the state at time $\\tau$, and therefore ranges over states. $\\mathbb{E}_{Q(s_\\tau \\mid \\pi)}\\left[ \\bullet \\right]$ is an expected value under the distribution over states. Since this is a discrete distribution, it can calculated as a sum over states:\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{\\text{num states}} Q(s_\\tau = s_j \\mid \\pi) \\bullet\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{s}_{\\pi \\tau}$ is a vector of the probabilities of each state given policy $\\pi$ and time $\\tau$. This is indeed the vector of probabilities that we want to sum over to get the expected value (after multiplying it element-wise by the $\\bullet$). $\\log$ is applied element-wise here. So, assume for instance that for some policy $\\pi_1$ [SFM: does the subscript denote a policy or a timestep?] and time $\\tau_1$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{s}_{\\pi_1 \\tau_1} &=\n",
    "\\begin{bmatrix}\n",
    "p(s_1|\\pi_1,\\tau_1) \\\\\n",
    "p(s_2|\\pi_1,\\tau_1) \n",
    "\\end{bmatrix}\\\\[0.5ex]\n",
    "&= \\begin{bmatrix}\n",
    "0.75 \\\\\n",
    "0.25\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "\\begin{align}\n",
    "& \\mathbf{s}_{\\pi_1 \\tau_1} \\cdot \\log \\mathbf{s}_{\\pi_1 \\tau_1} \\\\\n",
    "= & \\begin{bmatrix} 0.75 \\\\ 0.25  \\end{bmatrix} \\cdot \\log_2 \\left( \\begin{bmatrix} 0.75 \\\\ 0.25 \\end{bmatrix} \\right) \\\\[0.5ex]\n",
    "= & \\begin{bmatrix} 0.75 \\\\ 0.25  \\end{bmatrix} \\cdot \\begin{bmatrix} -0.415... \\\\ -2 \\end{bmatrix} \\\\[0.5ex]\n",
    "= & \\left(0.75 \\times -0.415...\\right) + \\left(0.25 \\times -2\\right) \\approx -0.811\n",
    "\\end{align}\n",
    "\n",
    "Once we calculate this single number for all timesteps, we just sum over timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now onto the second summand! The expectation is expressed in terms of the sufficient parameters of $Q$ as follows:\n",
    "\n",
    "$$\n",
    "\\sum_{\\tau=1}^t \\mathbb{E}_{Q( s_\\tau \\mid \\pi )Q(A)}\\left[ \\log P\\left( o_\\tau \\mid s_\\tau , A \\right) \\right]\n",
    "\\implies \\sum_{\\tau=1}^{t} o_\\tau \\cdot \\mathrm{\\textbf{log}}\\mathbf{A s_{\\pi\\tau}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$o_\\tau$ here is the observation that was in fact made at time $\\tau$ - we are not calculating expectations over that. First, note that from the generative model above we know that (where, as above, $o_\\tau$ and $s_\\tau$ are one-hot vectors, and $A$ is the likelihood matrix, and $\\log$ is applied component-wise):\n",
    "\n",
    "$$\n",
    "\\log P\\left( o_\\tau \\mid s_\\tau , A \\right) = o_\\tau \\cdot \\log A s_\\tau\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And therefore:\n",
    "\n",
    "\\begin{align}\n",
    "& \\mathbb{E}_{Q( s_\\tau \\mid \\pi )Q(A)}\\left[ \\log P\\left( o_\\tau \\mid s_\\tau , A \\right) \\right] \\\\\n",
    "= & \\sum_s \\sum_a \\left[ Q(A=a) Q( s_\\tau=s \\mid \\pi ) \\log P\\left( o_\\tau \\mid s, a \\right) \\right] \\\\\n",
    "= &  \\sum_s \\left[ Q( s_\\tau=s \\mid \\pi ) \\sum_a \\left[ Q(A=a) o_\\tau \\cdot \\log a s \\right] \\right] \\\\\n",
    "= & o_\\tau \\cdot \\sum_s \\left[ Q( s_\\tau=s \\mid \\pi ) \\sum_a \\left[ Q(A=a) \\log a \\right] s \\right] \\\\\n",
    "= & o_\\tau \\cdot \\sum_a \\left[ Q(A=a) \\log a \\right] \\sum_s \\left[ Q( s_\\tau=s \\mid \\pi )  s \\right] \\\\\n",
    "= & o_\\tau \\cdot \\mathrm{\\textbf{log}}\\mathbf{A} \\sum_s \\left[ Q( s_\\tau=s \\mid \\pi )  s \\right] \\\\\n",
    "= & o_\\tau \\cdot \\mathrm{\\textbf{log}}\\mathbf{A} \\mathbf{s_{\\pi\\tau}} \\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "$\\mathrm{\\textbf{log}}\\mathbf{A}$ is defined as the expected value under $Q(A)$ of the logarithm of $A$, $\\mathbf{E}_{Q(A)} \\left[ \\log(A) \\right]$, and it is an $n \\times m$ matrix. It is calculated with the digamma function $\\psi$: see expected value of the log of a Dirichlet distribution on wikipedia.\n",
    "\n",
    "The last thing to clarify for the second summand is why:\n",
    "\n",
    "$$\n",
    "\\mathbf{s_{\\pi\\tau}} = \\sum_s \\left[ Q( s_\\tau=s \\mid \\pi )  s \\right]\n",
    "$$\n",
    "\n",
    "To see why, recall that $s$ is not a number, but rather a one-hot vector. Therefore, the sum is a sum over one-hot vectors, where the 1 in each vector becomes the probability of the respective state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, onto the third summand! The identity is as follows:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{Q(s_1 \\mid \\pi)} \\left[ \\log P(s_1) \\right] & = \\sum_s \\left[ Q(s_1=s \\mid \\pi) \\log P(s) \\right] \\\\\n",
    "& = \\mathbf{s_{\\pi 1}} \\log D\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, the fourth summand:\n",
    "\n",
    "$$\n",
    "\\sum_{\\tau=2}^T \\mathbb{E}_{Q(s_\\tau \\mid \\pi) Q(s_{\\tau-1} \\mid \\pi)}\\left[ \\log P (s_\\tau \\mid s_{\\tau-1}, \\pi) \\right] \\implies \\sum_{\\tau=2}^T \\mathbf{s_{\\pi \\tau}} \\cdot \\log \\left( B_{\\pi_{\\tau-1}} \\right) \\mathbf{s_{\\pi \\tau-1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, this one looks a little bit nasty. Let's see:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{Q(s_\\tau \\mid \\pi) Q(s_{\\tau-1} \\mid \\pi)}\\left[ \\log P (s_\\tau \\mid s_{\\tau-1}, \\pi) \\right] & = \n",
    "\\sum_{s_i} \\sum_{s_j}\\left[ Q(s_\\tau = s_i \\mid \\pi) Q(s_{\\tau-1} = s_j \\mid \\pi)  \\log P (s_i \\mid s_j, \\pi) \\right] \\\\\n",
    "& = \\sum_{s_i} \\sum_{s_j} \\left[ Q(s_\\tau = s_i \\mid \\pi) Q(s_{\\tau-1} = s_j \\mid \\pi) s_i \\cdot \\log \\left( B_{\\pi_{\\tau-1}} \\right) s_j \\right] \\\\\n",
    "& = \\sum_{s_i} \\left[ Q(s_\\tau = s_i \\mid \\pi) s_i \\right] \\cdot \\log \\left( B_{\\pi_{\\tau-1}} \\right) \\sum_{s_j} \\left[ Q(s_{\\tau-1} = s_j \\mid \\pi) s_j \\right]  \\\\\n",
    "& = \\mathbf{s_{\\pi \\tau}} \\cdot \\log \\left( B_{\\pi_{\\tau-1}} \\right) \\mathbf{s_{\\pi \\tau-1}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to sum up, what do we have? We have a way of calculating the free energy of a parameterization of $Q$ in terms of the probabilities of each state at each timestep (given a policy). Since perception is conceptualized as estimation of the true state, we _almost_ have a model of perception!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_free_energy_states(s_pi, T, logA, D, B, pi, o_history):\n",
    "    \"\"\"\n",
    "        Equation (7), p12 of Da Costa et al (2020).\n",
    "        Variational free energy of Q, with respect to P, conditioned on policy π.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## first summand\n",
    "    first_summand = np.diag(s_pi @ np.log(s_pi).T)\n",
    "    \n",
    "    ## second summand (TODO: vectorize)\n",
    "    second_summand = np.sum([(logA @ s_pi[t].reshape(-1,1))[o] for t, o in enumerate(o_history)])\n",
    "    \n",
    "    ## third summand\n",
    "    third_summand = s_pi[0] @ np.log(D).reshape(-1,1)\n",
    "    \n",
    "    ## fourth summand (TODO: vectorize)\n",
    "    fourth_summand = np.sum([s_pi[tau] @ np.log(B[pi[tau]]) @ s_pi[tau-1] for tau in range(1,T)])\n",
    "    \n",
    "    return first_summand - second_summand - third_summand - fourth_summand\n",
    "\n",
    "\n",
    "def generate_random_s_pi(T, n_states):\n",
    "    \"\"\"\n",
    "        For each timestep, get probabilities of states.\n",
    "    \"\"\"\n",
    "    \n",
    "    unnorm = np.random.uniform(size=(T, n_states))\n",
    "    return unnorm / np.sum(unnorm, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of calculation of the free energy. Note that the result is an array with length T. This is because we are approximating, with the Q distribution, one probability vector over states for each timestep. So this is a measure, in free-energy terms, of how close our approximation with $s_\\pi$ gets to the true posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.99479278, 15.02718358, 15.0108749 , 14.99573584, 15.19782351,\n",
       "       15.01811514, 15.17491113, 15.03003093, 14.98353729, 15.03984324])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1. Initialise generative model\n",
    "n_states, n_obs, T, B, pi, A, D, a = initialise()\n",
    "\n",
    "## 2. Random state probabilities per timestep\n",
    "s_pi = generate_random_s_pi(T, n_states)\n",
    "\n",
    "## 3. Generate history of states and observations from model\n",
    "o_history, s_history = generate_history(n_states, n_obs, T, B, pi, A, D)\n",
    "\n",
    "## 4. SFM: need to calculate logA\n",
    "a_0 = np.sum(a, axis=1, keepdims=True)\n",
    "logA = digamma(a) - digamma(a_0)\n",
    "\n",
    "## 5. Calculate free energy for the first five entries\n",
    "o_history_until_t = o_history[:5]\n",
    "calculate_free_energy_states(s_pi, T, logA, D, B, pi, o_history_until_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's missing? Well, just knowing the free energy of a set of parameters is not enough. What we do in perception is find the set of parameters that _minimizes_ the free energy. This is done by gradient descent. Now that we have the free energy of a parameterization of Q consisting of the probabilities of all states for each timestep, the authors calculate the gradient wrt to each timestep, which is a vector for each timestep. The total gradient therefore (as it should) has the same shape as the parameters we are updating: a $T \\times n$ matrix.\n",
    "\n",
    "I am not going to repeat the formula in the paper, but rather just implement it in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_free_energy_perception(s_pi, a, D, o_history_until_t, pi, B):\n",
    "    \"\"\"\n",
    "    Function to calculate the gradient of the sufficient parameters for the Q distribution\n",
    "    over states at each timestep, s_pi.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    s_pi: array\n",
    "        An array with shape (T, n). Encodes the current estimation of the probability \n",
    "        of each state at each timestep.\n",
    "    a: array\n",
    "        An array with shape (n, m). Encodes the hyperprior over transition matrices in Q.\n",
    "        Represented with a bold \"a\" in the paper.\n",
    "    D: array\n",
    "        Array of length n, encodes the prior probabilities of each state on the first step.\n",
    "    o_history_until_t: array\n",
    "        Vector of observations of length t.\n",
    "    pi: array\n",
    "        Vector of action indices.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        The gradient of s_pi at the current point.\n",
    "    \"\"\"\n",
    "    \n",
    "    t = len(o_history_until_t)\n",
    "    \n",
    "    a_0 = np.sum(a, axis=1, keepdims=True)\n",
    "    logA = digamma(a) - digamma(a_0)\n",
    "    logD = np.log(D)\n",
    "    \n",
    "    conditional_part = np.zeros(shape=(s_pi.shape))\n",
    "    \n",
    "    ## TODO: check if this loop is a bottleneck & vectorize if so. \n",
    "    ##  Otherwise, keep it: it's clearer this way.\n",
    "    for tau in range(t-1):\n",
    "        #s_pi_tau = s_pi[tau] # SFM comment out unused variable\n",
    "        \n",
    "        ## state-state transitions entailed by current policy\n",
    "        log_B_pi_tau = np.log(B[pi[tau]]) \n",
    "        \n",
    "        ## state-state transitions entailed by previous policy\n",
    "        log_B_pi_tau_minus_one = np.log(B[pi[tau-1]])\n",
    "        \n",
    "        #log_s_pi_tau = np.log(s_pi_tau) # SFM comment out unused variable\n",
    "        \n",
    "        ## this is nonsense, but also it's not used, when tau=0\n",
    "        s_pi_tau_minus_one = s_pi[tau-1]\n",
    "        \n",
    "        s_pi_tau_plus_one = s_pi[tau+1]\n",
    "        \n",
    "        ## Equation (8) page 12\n",
    "        ## Looks like indices of tau are reduced by 1\n",
    "        if tau == 0:\n",
    "            ## NOTE: o_tau is the index of o at time tau, rather than the one-hot like in the paper\n",
    "            o_tau = o_history_until_t[tau] \n",
    "            x = logA[o_tau] +\\\n",
    "                s_pi_tau_plus_one @ log_B_pi_tau +\\\n",
    "                logD\n",
    "                \n",
    "        elif 0 < tau and tau < t:\n",
    "            o_tau = o_history_until_t[tau]\n",
    "            x = logA[o_tau] +\\\n",
    "                s_pi_tau_plus_one @ log_B_pi_tau +\\\n",
    "                log_B_pi_tau_minus_one @ s_pi_tau_minus_one\n",
    "                \n",
    "        else:\n",
    "            x = s_pi_tau_plus_one @ log_B_pi_tau +\\\n",
    "                log_B_pi_tau_minus_one @ s_pi_tau_minus_one\n",
    "        \n",
    "        conditional_part[tau] = x\n",
    "    \n",
    "    ## Full form of equation (8)\n",
    "    return 1 + np.log(s_pi) - conditional_part\n",
    "\n",
    "\n",
    "def gradient_descent(n_states, n_obs, T, B, pi, A, D, a, o_history_until_t, s_history,\n",
    "                     learning_rate=0.05, feedback=[]):\n",
    "    \"\"\"\n",
    "        Perform gradient descent on s_pi.\n",
    "        Equation (9) page 12\n",
    "    \"\"\"\n",
    "    \n",
    "    ## 1. Initialise\n",
    "    a_0 = np.sum(a, axis=1, keepdims=True)\n",
    "    logA = digamma(a) - digamma(a_0)\n",
    "    \n",
    "    ## Be careful here: approximate_a is represented as the bold a in the paper. \n",
    "    ## Part of Q distribution!\n",
    "    fixed_input = {\n",
    "            \"a\": a, \n",
    "            \"D\": D, \n",
    "            \"o_history_until_t\": o_history_until_t, \n",
    "            \"pi\": pi,\n",
    "            \"B\": B\n",
    "            }\n",
    "\n",
    "    ## 2. Set initial guess - can be improved\n",
    "    s_pi = generate_random_s_pi(T, n_states)\n",
    "    \n",
    "    t = len(o_history_until_t)\n",
    "    \n",
    "    ## 3. Repeatedly move down gradient\n",
    "    for i in range(10):\n",
    "        gradient = gradient_free_energy_perception(s_pi, **fixed_input)\n",
    "\n",
    "        if not i%1:\n",
    "            if \"FE\" in feedback:\n",
    "                print(\"FE: \",\n",
    "                      calculate_free_energy_states(s_pi, T, logA, D, B, pi, o_history_until_t))\n",
    "            if \"s_pi\" in feedback:\n",
    "                print(\"s_pi\\n\", np.round(s_pi[:10], 3))\n",
    "            if \"gradient\" in feedback:\n",
    "                print(\"gradient \\n\", gradient[:10], \"\\n\\n\")\n",
    "            if \"accuracy\" in feedback:\n",
    "                print(\"Accuracy on observed: \", \n",
    "                      ## np.argmax(s_pi, axis=1) is the vector of predictions on the hidden states\n",
    "                      1-(np.sum(np.absolute(s_history[:t] - np.argmax(s_pi, axis=1)[:t]))/t))\n",
    "            if \"loss\" in feedback:\n",
    "                print(\"Loss: \", -np.sum(np.log(s_pi[np.arange(len(s_pi)), s_history])))\n",
    "\n",
    "        s_pi = softmax(s_pi - learning_rate * gradient, axis=1)\n",
    "        \n",
    "    return s_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work through a simple example with a generative model that has a predictable behaviour. First define the basic parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 80\n",
    "n_states, n_obs, n_actions = 2,2,2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability vector for first state. Almost always starts with state 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.array([\n",
    "    0.9, 0.1\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always perform action 0, i.e. only consider the first element of B for all timesteps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = [0]*T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the state-state transitions.\n",
    "\n",
    "In this case, both actions have the same effect: the state remains the same with high probability:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.array([\n",
    "    [[0.9, 0.1],\n",
    "     [0.1, 0.9]],\n",
    "    \n",
    "    [[0.9, 0.1],\n",
    "     [0.1, 0.9]]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the state-observation transitions.\n",
    "\n",
    "Again, A encodes the likelihood of producting each observation (row) given the state (column). If the state is 0, it usually produces observation 0. If the state is 1, it usually produces observation 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [0.9, 0.1],\n",
    "    [0.1, 0.9]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a$ here should be the expected value of A. I'll just set it to A for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, generate the history and do the gradient descent.\n",
    "\n",
    "Remember, these are considered to be the actual histories of observations and states. We use components of the generative *model* because they are assumed to be the same as their counterpart features in the generative *process*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on observed:  0.6285714285714286\n",
      "Accuracy on observed:  0.8571428571428572\n",
      "Accuracy on observed:  0.8571428571428572\n",
      "Accuracy on observed:  0.8571428571428572\n",
      "Accuracy on observed:  0.8571428571428572\n",
      "Accuracy on observed:  0.8571428571428572\n",
      "Accuracy on observed:  0.8571428571428572\n",
      "Accuracy on observed:  0.8571428571428572\n",
      "Accuracy on observed:  0.8571428571428572\n",
      "Accuracy on observed:  0.8571428571428572\n"
     ]
    }
   ],
   "source": [
    "o_history, s_history = generate_history(n_states, n_obs, T, B, pi, A, D)\n",
    "\n",
    "t = 70 # what's the present timestep?\n",
    "o_history_until_t = o_history[:t]\n",
    "\n",
    "s_pi = gradient_descent(\n",
    "    n_states, n_obs, T, B, pi, A, D, a, o_history_until_t, s_history, learning_rate=0.1, feedback=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect of the parameters set above is that the generative model is quite predictable in general.\n",
    "\n",
    "However, since in general the observations track the state so well, the agent is fooled when the observation happens to be different from the real state, because the observation is guessed rather than the state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s history beginning: [0 0 0 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1 1]\n",
      "o history beginning: [0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 1 1 1]\n",
      "Predicted states:    [0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(f\"s history beginning: {np.array(s_history[:20])}\")\n",
    "print(f\"o history beginning: {np.array(o_history[:20])}\")\n",
    "print(f\"Predicted states:    {np.argmax(s_pi, axis=1)[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a slightly more complicated case, the agent is capable of inferring the the observation is different from the state, even with pretty uninformative observations. To do so, the agent has to know that whenever a certain action is performed, a certain state follows. So we need to change $A$, $B$ and $\\pi$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on observed:  0.5857142857142856\n",
      "Accuracy on observed:  1.0\n",
      "Accuracy on observed:  1.0\n",
      "Accuracy on observed:  1.0\n",
      "Accuracy on observed:  1.0\n",
      "Accuracy on observed:  1.0\n",
      "Accuracy on observed:  1.0\n",
      "Accuracy on observed:  1.0\n",
      "Accuracy on observed:  1.0\n",
      "Accuracy on observed:  1.0\n"
     ]
    }
   ],
   "source": [
    "## 2. in new policy, alternative first and second moves\n",
    "pi = [0, 1]*(T//2)\n",
    "\n",
    "## 3. state-state transitions\n",
    "## when action 0 is performed, system always tends to go to state 0.\n",
    "## when action 1 is performed, system always tends to go to state 1.\n",
    "B = np.array([\n",
    "    [[0.999, 0.999],\n",
    "     [0.001, 0.001]],\n",
    "\n",
    "    [[0.001, 0.001],\n",
    "     [0.999, 0.999]]\n",
    "])\n",
    "\n",
    "## 3. state-observation transitions make the observation less informative\n",
    "A = np.array([\n",
    "    [0.6, 0.4],\n",
    "    [0.4, 0.6]\n",
    "])\n",
    "a = A\n",
    "\n",
    "## 4. generate history\n",
    "o_history, s_history = generate_history(n_states, n_obs, T, B, pi, A, D)\n",
    "t = 70 # what's the present timestep?\n",
    "o_history_until_t = o_history[:t]\n",
    "s_pi = gradient_descent(n_states, n_obs, T, B, pi, A, D, a, o_history_until_t, \n",
    "                        s_history, learning_rate=0.1, feedback=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the agent is guessing the following: \n",
    "1. start with state 0, because of the way $D$ was specified\n",
    "2. guess state 0 (because action 0 was performed at time 0, leading to state 0 according to B)\n",
    "3. guess state 1 (because action 1 was performed at time 1, leading to state 1 according to B)\n",
    "4. Repeat!\n",
    "\n",
    "Importantly, because $A$ doesn't give much information anymore, the agent disregards the observations in the inference of the states and instead uses the relation between the actions and the resulting states. This can be seen by printing the first few passages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s history beginning: [0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0]\n",
      "o history beginning: [0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 0 1 0 1 1]\n",
      "Actions performed:   [0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1]\n",
      "Predicted states:    [0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"s history beginning: {np.array(s_history[:20])}\")\n",
    "print(f\"o history beginning: {np.array(o_history[:20])}\")\n",
    "print(f\"Actions performed:   {np.array(pi[:20])}\")\n",
    "print(f\"Predicted states:    {np.argmax(s_pi, axis=1)[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it's fun to just see how the agent does with totally random histories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on observed:  0.5714285714285714\n",
      "Loss:  78.54709530604588\n",
      "Accuracy on observed:  0.5714285714285714\n",
      "Loss:  68.35776256861323\n",
      "Accuracy on observed:  0.5714285714285714\n",
      "Loss:  68.02033026198107\n",
      "Accuracy on observed:  0.5571428571428572\n",
      "Loss:  68.35604142942185\n",
      "Accuracy on observed:  0.5857142857142856\n",
      "Loss:  68.63252806349504\n",
      "Accuracy on observed:  0.5857142857142856\n",
      "Loss:  68.79526710953267\n",
      "Accuracy on observed:  0.6\n",
      "Loss:  68.88184725294737\n",
      "Accuracy on observed:  0.5857142857142856\n",
      "Loss:  68.92605650679457\n",
      "Accuracy on observed:  0.5714285714285714\n",
      "Loss:  68.94821967450032\n",
      "Accuracy on observed:  0.5714285714285714\n",
      "Loss:  68.95923671029115\n",
      "s history beginning: [1 0 0 1 1 1 0 1 0 0 0 1 0 1 0 0 1 1 0 1]\n",
      "o history beginning: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Actions performed:   [1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 0 0]\n",
      "Predicted states:    [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "n_states, n_obs, T, B, pi, A, D, a = initialise(T=100)\n",
    "o_history, s_history = generate_history(n_states, n_obs, T, B, pi, A, D)\n",
    "\n",
    "approximate_a = a # for the moment assume that they are the same\n",
    "num_observed = 70 # what's the present timestep?\n",
    "o_history_until_t = o_history[:num_observed]\n",
    "\n",
    "# print(np.column_stack((o_history, s_history)))\n",
    "s_pi = gradient_descent(n_states, n_obs, T, B, pi, A, D, approximate_a, o_history_until_t, s_history, \n",
    "                 learning_rate=0.01, feedback=[\"accuracy\", \"loss\"])\n",
    "\n",
    "print(f\"s history beginning: {np.array(s_history[:20])}\")\n",
    "print(f\"o history beginning: {np.array(o_history[:20])}\")\n",
    "print(f\"Actions performed:   {np.array(pi[:20])}\")\n",
    "print(f\"Predicted states:    {np.argmax(s_pi, axis=1)[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.43092966, 0.97475326],\n",
       "        [0.56907034, 0.02524674]],\n",
       "\n",
       "       [[0.45806247, 0.57671185],\n",
       "        [0.54193753, 0.42328815]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00729684, 0.0220927 ],\n",
       "       [0.99270316, 0.9779073 ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.32911382, 0.67088618])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.22620098, 0.75954744],\n",
       "       [1.64017518, 0.78255023]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for x, xname in zip([T, B, pi, A, D, a], [\"T\", \"B\", \"pi\", \"A\", \"D\", \"a\"]):\n",
    "    print(xname, \":\")\n",
    "    display(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\textbf{a}$: As explained above, an $n \\times m$ matrix, encoding the hyperparameters from which the approximate likelihood matrix is sampled. Each column is an $\\alpha$ parameter for a Dirichlet distribution. \n",
    "- $\\textbf{a_0}$: An $n \\times m$ matrix, which consists of one column vector with $n$ components repeated $m$ times. The single value of each row is calculated by summing all the columns of $\\textbf{a}$.\n",
    "- $\\textbf{A}$ (Note: different from $A$!): Defined in the paper as the expectation under $Q$ of $A$, i.e. $\\sum_a Q(A=a)\\left[ a \\right]$. Basically it is $\\textbf{a}$, but with normalized rows. This normalization is done by component-wise division by the corresponding elements of $\\textbf{a_0}$\n",
    "\n",
    "The fundamental thing to notice is that expected free energy does not concern all timesteps, but rather only a specific one, which is usually set to the last timestep $T$. This is not emphasized very much in the paper so I was confused about dimensions when I started implementing this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_H(A_bold):\n",
    "    \"\"\"\n",
    "        a is n x m, therefore A.T @ np.log(A) is m x m, and its diagonal is length m.\n",
    "        which is the number of states.\n",
    "        this is right, considering it multiplies s_pi_tau, which contains probabilities of states.\n",
    "    \"\"\"\n",
    "    \n",
    "    return - np.diag(A_bold.T @ np.log(A_bold))\n",
    "\n",
    "def calculate_W(a_bold, a_0_bold):\n",
    "    return 0.5 * ( (1/a_bold) - (1/a_0_bold) )\n",
    "\n",
    "def calculate_ambiguity(s_pi_tau, A_bold):\n",
    "    ## they are both vectors, so this is simply the dot product\n",
    "    return calculate_H(A_bold) @ s_pi_tau\n",
    "\n",
    "def calculate_risk(s_pi_tau, C):\n",
    "    return s_pi_tau @ (np.log(s_pi_tau) - np.log(C))\n",
    "\n",
    "def calculate_novelty(A_bold, s_pi_tau, a_bold, a_0_bold):\n",
    "    return (A_bold @ s_pi_tau.T) @ (calculate_W(a_bold, a_0_bold) @ s_pi_tau)\n",
    "\n",
    "def expected_free_energy(s_pi_tau, A_bold, C, a_bold, a_0_bold):\n",
    "    \"\"\"\n",
    "        Expected free energy concerns only a specific timestep, \n",
    "         which is usually set to the last timestep T.\n",
    "    \"\"\"\n",
    "    \n",
    "    ambiguity = calculate_ambiguity(s_pi_tau, A_bold)\n",
    "    risk = calculate_risk(s_pi_tau, C)\n",
    "    novelty = calculate_novelty(A_bold, s_pi_tau, a_bold, a_0_bold)\n",
    "    return ambiguity + risk - novelty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: To calculate the expected free energy under a policy, I need the approximate prior over states at each timestep, s_pi_tau. However, I cannot estimate the probability of the states at each timestep (s_pi_tau) without minimizing the free energy over states. The gradient is minimized with respect to a certain history. But producing a history requires a policy!\n",
    "\n",
    "The solution to this lies in the concept of _active inference_: \n",
    "- First sample an initial state, for which no policy is required\n",
    "- Then, sample an initial observation.\n",
    "- Based on the initial observation, do free energy minimization on the hidden states and get an s_pi\n",
    "- Based on the obtained s_pi, do free energy minimization on the policies and pick an action.\n",
    "- Perform the action, which causes another state, which causes another observation\n",
    "- Repeat!\n",
    "\n",
    "The only thing to be careful with is to restrict the set of policies to consider to the ones that are compatible with actions performed in the past: can't change the past!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_policy(n_states, n_obs, n_actions, i, T, B, A_bold, C, D, \n",
    "                a_bold, a_0_bold, o_history, s_history, pi):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    pi: array\n",
    "        The policy decided in the previous timestep. The picked policy has to be consistent with the\n",
    "        previously adopted policies up to the present time.\n",
    "    i: integer\n",
    "        Index of present time. Starts with 0 for the first timestep.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## 1. history of the actions performed until now\n",
    "    ##   Ensures we pick a policy that doesn't entail changing the past\n",
    "    pi_history = pi[:i]\n",
    "    \n",
    "    Gs = []\n",
    "    PI = []\n",
    "    \n",
    "    ## 2. loop over possible continuations of the policy\n",
    "    for remaining_pi in product(np.arange(n_actions), repeat=T-i):\n",
    "        \n",
    "        pi = pi_history + remaining_pi\n",
    "        s_pi = gradient_descent(n_states, n_obs, T, B, pi, A_bold, D, a_bold, o_history, s_history)\n",
    "        # s_pi[-1] because free energy of policy is minimized wrt to last timestep\n",
    "        G_pi = expected_free_energy(s_pi[-1], A_bold, C, a_bold, a_0_bold)\n",
    "        Gs.append(G_pi)\n",
    "        PI.append(pi)\n",
    "    \n",
    "    ## 3. output\n",
    "    print(\"time \", i)\n",
    "    print(\"pi:   \", np.array(pi))\n",
    "    print(\"s_pi: \", np.argmax(s_pi, axis=1), \"\\n\")\n",
    "    Q_pi = softmax(-np.array(Gs))\n",
    "    index_pi = np.random.choice(np.arange(len(PI)), p=Q_pi)\n",
    "    \n",
    "    return PI[index_pi]\n",
    "\n",
    "\n",
    "def generate_history_with_active_inference(\n",
    "        n_states, \n",
    "        n_obs, \n",
    "        n_actions, \n",
    "        T, \n",
    "        B, \n",
    "        A, \n",
    "        C, \n",
    "        D, \n",
    "        a # SFM added\n",
    "        ):\n",
    "\n",
    "    ## 1. Initialise\n",
    "    ## assume for simplicity that the agent's prior about A accurately reflect reality\n",
    "    a_bold = a\n",
    "    \n",
    "    ## expected value of A, calculated by normalizing the columns of a\n",
    "    A_bold = a / np.sum(a, axis=0)\n",
    "    a_0_bold = np.sum(a_bold, axis=1, keepdims=True)\n",
    "    \n",
    "    s = np.random.choice(n_states, p=D)\n",
    "    \n",
    "    ## I can initialize it as an empty tuples, it doesn't matter because\n",
    "    ## on the first round none of it is used.\n",
    "    pi = ()\n",
    "\n",
    "    s_history, o_history = [], []\n",
    "    \n",
    "    ## 2. in a loop which models the time steps...\n",
    "    for i in range(T):\n",
    "        \n",
    "        s_history.append(s)\n",
    "        \n",
    "        ## ...sample observation given A and state s...\n",
    "        o = np.random.choice(n_obs, p=A[:,s])\n",
    "        o_history.append(o)\n",
    "        \n",
    "        ## ...update policy by performing active inference:\n",
    "        ##  actions influence state at i+1\n",
    "        ##  action at timestep T doesn't matter, because it could only influence state T+1\n",
    "        product(np.arange(n_actions), repeat=T)\n",
    "        pi = pick_policy(n_states, n_obs, n_actions, i, T, B, A_bold, C, D, a_bold, a_0_bold,\n",
    "                               o_history, s_history, pi)\n",
    "        \n",
    "        ## ...update state s_i given policy, B, and previous state\n",
    "        ## not appended on the last timestep, because \n",
    "        ##  it always depends on decision in previous timestep\n",
    "        ## so this would be at T+1, in the list index i+1\n",
    "        s = np.random.choice(n_states, p=B[pi[i],:,s])\n",
    "        \n",
    "    return o_history, s_history, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 10\n",
    "\n",
    "n_states, n_obs, n_actions = 2,3,2\n",
    "\n",
    "D = np.array([\n",
    "    0.9, 0.1\n",
    "])\n",
    "\n",
    "B = np.array([\n",
    "    [[0.999, 0.999],\n",
    "     [0.001, 0.001]],\n",
    "    \n",
    "    [[0.001, 0.001],\n",
    "     [0.999, 0.999]]\n",
    "])\n",
    "\n",
    "A = np.array([\n",
    "    [0.8, 0.1],\n",
    "    [0.1, 0.8],\n",
    "    [0.1, 0.1]\n",
    "])\n",
    "\n",
    "a = A\n",
    "\n",
    "# preference expressed in terms of states (formula is different when preference is wrt outcomes)\n",
    "C = np.array([0.999, 0.001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time  0\n",
      "pi:    [1 1 1 1 1 1 1 1 1 1]\n",
      "s_pi:  [1 0 1 0 0 0 0 1 0 1] \n",
      "\n",
      "time  1\n",
      "pi:    [1 1 1 1 1 1 1 1 1 1]\n",
      "s_pi:  [0 0 1 0 0 0 0 0 0 1] \n",
      "\n",
      "time  2\n",
      "pi:    [1 1 1 1 1 1 1 1 1 1]\n",
      "s_pi:  [0 1 1 0 1 0 0 0 1 0] \n",
      "\n",
      "time  3\n",
      "pi:    [1 1 1 1 1 1 1 1 1 1]\n",
      "s_pi:  [0 1 1 0 0 1 0 1 1 0] \n",
      "\n",
      "time  4\n",
      "pi:    [1 1 1 0 1 1 1 1 1 1]\n",
      "s_pi:  [0 1 1 1 1 1 0 0 1 0] \n",
      "\n",
      "time  5\n",
      "pi:    [1 1 1 0 1 1 1 1 1 1]\n",
      "s_pi:  [0 1 1 1 0 0 0 1 0 0] \n",
      "\n",
      "time  6\n",
      "pi:    [1 1 1 0 1 1 1 1 1 1]\n",
      "s_pi:  [0 1 1 1 0 1 0 0 0 1] \n",
      "\n",
      "time  7\n",
      "pi:    [1 1 1 0 1 1 0 1 1 1]\n",
      "s_pi:  [0 1 1 1 0 1 1 0 0 0] \n",
      "\n",
      "time  8\n",
      "pi:    [1 1 1 0 1 1 0 0 1 1]\n",
      "s_pi:  [0 1 1 1 0 1 1 0 1 1] \n",
      "\n",
      "time  9\n",
      "pi:    [1 1 1 0 1 1 0 0 0 1]\n",
      "s_pi:  [0 1 1 1 0 1 1 0 0 1] \n",
      "\n",
      "s history:  [0, 1, 1, 1, 0, 1, 1, 0, 0, 0]\n",
      "pi:         (1, 1, 1, 0, 1, 1, 0, 0, 0, 0)\n",
      "o history:  [2, 1, 1, 1, 0, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "o_history, s_history, pi = generate_history_with_active_inference(\n",
    "                                    n_states, \n",
    "                                    n_obs, \n",
    "                                    n_actions, \n",
    "                                    T, \n",
    "                                    B, \n",
    "                                    A, \n",
    "                                    C, \n",
    "                                    D,\n",
    "                                    a # SFM added\n",
    "                                    )\n",
    "\n",
    "print(\"s history: \", s_history)\n",
    "print(\"pi:        \", pi)\n",
    "print(\"o history: \", o_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
